{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LEED Knowledge Graph\n",
    "\n",
    "This notebook demonstrates how to build a knowledge graph from LEED materials (PDFs and Excel files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from kg_extractor import KnowledgeGraphExtractor\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Knowledge Graph Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEEDKnowledgeGraphBuilder:\n",
    "    def __init__(self, config_path: str = \"../config.yaml\"):\n",
    "        \"\"\"Initialize the knowledge graph builder.\"\"\"\n",
    "        self.kg_extractor = KnowledgeGraphExtractor(config_path)\n",
    "        self.all_entities = {}  # Use dict for deduplication\n",
    "        self.all_relations = []\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[str]:\n",
    "        \"\"\"Extract text and images from PDF files.\"\"\"\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "        text_chunks = []\n",
    "        \n",
    "        try:\n",
    "            # Open PDF\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            for page_num, page in enumerate(doc):\n",
    "                # Extract text\n",
    "                text = page.get_text()\n",
    "                if text.strip():\n",
    "                    text_chunks.append(text)\n",
    "                \n",
    "                # Extract images\n",
    "                image_list = page.get_images()\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    \n",
    "                    # Convert to PIL Image\n",
    "                    image = Image.open(io.BytesIO(image_bytes))\n",
    "                    \n",
    "                    # TODO: Replace with actual vision model call\n",
    "                    logger.info(f\"Found image on page {page_num + 1}, image {img_index + 1}\")\n",
    "            \n",
    "            return text_chunks\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_from_excel(self, excel_path: str) -> List[str]:\n",
    "        \"\"\"Extract information from Excel files.\"\"\"\n",
    "        logger.info(f\"Processing Excel: {excel_path}\")\n",
    "        statements = []\n",
    "        \n",
    "        try:\n",
    "            # Read all sheets\n",
    "            excel_file = pd.ExcelFile(excel_path)\n",
    "            \n",
    "            for sheet_name in excel_file.sheet_names:\n",
    "                df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "                \n",
    "                # Convert each row to natural language\n",
    "                for _, row in df.iterrows():\n",
    "                    if 'fixture_type' in df.columns and 'flow_rate' in df.columns:\n",
    "                        statement = f\"The baseline flow rate for {row['fixture_type']} is {row['flow_rate']} gpm\"\n",
    "                        statements.append(statement)\n",
    "                    else:\n",
    "                        statement = \" \".join(f\"{col}: {val}\" for col, val in row.items())\n",
    "                        statements.append(statement)\n",
    "            \n",
    "            return statements\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing Excel {excel_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_text_chunk(self, text: str):\n",
    "        \"\"\"Process a single text chunk and accumulate entities and relations.\"\"\"\n",
    "        entities, relations = self.kg_extractor.process(text)\n",
    "        \n",
    "        # Deduplicate entities\n",
    "        for entity in entities:\n",
    "            self.all_entities[entity['id']] = entity\n",
    "        \n",
    "        # Add relations\n",
    "        self.all_relations.extend(relations)\n",
    "    \n",
    "    def build_knowledge_graph(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Build knowledge graph from all files in the input directory.\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Process all files\n",
    "        for file_path in input_path.glob('**/*'):\n",
    "            if file_path.suffix.lower() == '.pdf':\n",
    "                text_chunks = self.extract_text_from_pdf(str(file_path))\n",
    "                for chunk in text_chunks:\n",
    "                    self.process_text_chunk(chunk)\n",
    "            \n",
    "            elif file_path.suffix.lower() in ['.xlsx', '.xls']:\n",
    "                statements = self.extract_from_excel(str(file_path))\n",
    "                for statement in statements:\n",
    "                    self.process_text_chunk(statement)\n",
    "        \n",
    "        # Save results\n",
    "        self._save_results(output_path)\n",
    "        \n",
    "        logger.info(f\"Knowledge graph built successfully:\")\n",
    "        logger.info(f\"- {len(self.all_entities)} unique entities\")\n",
    "        logger.info(f\"- {len(self.all_relations)} relations\")\n",
    "    \n",
    "    def _save_results(self, output_path: Path):\n",
    "        \"\"\"Save entities and relations to JSON files.\"\"\"\n",
    "        # Save entities\n",
    "        with open(output_path / 'entities.json', 'w') as f:\n",
    "            json.dump(list(self.all_entities.values()), f, indent=2)\n",
    "        \n",
    "        # Save relations\n",
    "        with open(output_path / 'relations.json', 'w') as f:\n",
    "            json.dump(self.all_relations, f, indent=2)\n",
    "    \n",
    "    def visualize_graph(self, max_nodes: int = 100):\n",
    "        \"\"\"Create a visualization of the knowledge graph.\"\"\"\n",
    "        # Create a new graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes (entities)\n",
    "        for entity in list(self.all_entities.values())[:max_nodes]:\n",
    "            G.add_node(entity['id'], label=entity['text'], type=entity['type'])\n",
    "        \n",
    "        # Add edges (relations)\n",
    "        for relation in self.all_relations:\n",
    "            if relation['source'] in G and relation['target'] in G:\n",
    "                G.add_edge(relation['source'], relation['target'], label=relation['type'])\n",
    "        \n",
    "        # Create the visualization\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        pos = nx.spring_layout(G)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=1000, alpha=0.8)\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, arrows=True)\n",
    "        \n",
    "        # Add labels\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "        \n",
    "        # Add edge labels\n",
    "        edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
    "        \n",
    "        plt.title(\"LEED Knowledge Graph\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the builder\n",
    "builder = LEEDKnowledgeGraphBuilder()\n",
    "\n",
    "# Build the knowledge graph\n",
    "builder.build_knowledge_graph(\n",
    "    input_dir='../data/leed_materials',\n",
    "    output_dir='../output'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph (showing first 100 nodes for clarity)\n",
    "builder.visualize_graph(max_nodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved entities and relations\n",
    "with open('../output/entities.json', 'r') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "with open('../output/relations.json', 'r') as f:\n",
    "    relations = json.load(f)\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "entities_df = pd.DataFrame(entities)\n",
    "relations_df = pd.DataFrame(relations)\n",
    "\n",
    "# Display entity type distribution\n",
    "print(\"Entity Type Distribution:\")\n",
    "print(entities_df['type'].value_counts())\n",
    "\n",
    "print(\"\\nRelation Type Distribution:\")\n",
    "print(relations_df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Graph Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_entity(entity_id: str):\n",
    "    \"\"\"Explore an entity and its relationships.\"\"\"\n",
    "    # Get entity details\n",
    "    entity = next((e for e in entities if e['id'] == entity_id), None)\n",
    "    if not entity:\n",
    "        print(f\"Entity {entity_id} not found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Entity: {entity['text']} (Type: {entity['type']})\")\n",
    "    \n",
    "    # Find related entities\n",
    "    outgoing = [r for r in relations if r['source'] == entity_id]\n",
    "    incoming = [r for r in relations if r['target'] == entity_id]\n",
    "    \n",
    "    print(\"\\nOutgoing relationships:\")\n",
    "    for rel in outgoing:\n",
    "        target = next((e['text'] for e in entities if e['id'] == rel['target']), None)\n",
    "        print(f\"- {rel['type']} -> {target}\")\n",
    "    \n",
    "    print(\"\\nIncoming relationships:\")\n",
    "    for rel in incoming:\n",
    "        source = next((e['text'] for e in entities if e['id'] == rel['source']), None)\n",
    "        print(f\"- {source} -> {rel['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explore a specific entity\n",
    "# Replace 'CREDIT_1_1' with an actual entity ID from your graph\n",
    "explore_entity('CREDIT_1_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
